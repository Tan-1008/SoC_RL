{"cells":[{"cell_type":"markdown","metadata":{"id":"5YOh6EaZULOj"},"source":["## PPO for Car Racing from Scratch\n","\n","This notebook implements the Proximal Policy Optimization (PPO) algorithm to train an agent for the `CarRacing-v2` environment in OpenAI Gym. The code is broken down into logical sections, mirroring the structure of a typical machine learning project in a Colab environment."]},{"cell_type":"markdown","metadata":{"id":"o8xomSlnULOl"},"source":["### 1. Installation\n","\n","First, we need to install the required libraries. `box2d-py` requires some system-level dependencies to be built correctly. We'll install those first using `apt-get` and then proceed with the `pip` installations."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZcP7FMRsULOl","executionInfo":{"status":"ok","timestamp":1753899572511,"user_tz":-330,"elapsed":27538,"user":{"displayName":"Tanmay Sinha","userId":"08721783457593017863"}},"outputId":"4b7cecc4-e0bf-4ca8-e531-531e60c9962a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.2.0)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.0.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.14.1)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n","Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.3.5)\n","Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n","Requirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.3.1)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (2.37.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from imageio) (2.0.2)\n","Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio) (11.3.0)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.12.0.88)\n","Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.59.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n","Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"]}],"source":["# Make sure to run this cell in your environment (e.g., Google Colab)\n","# First, install system dependencies for building box2d-py, which can cause errors.\n","!apt-get update > /dev/null 2>&1\n","!apt-get install -y swig > /dev/null 2>&1\n","\n","# Now, install the Python packages\n","!pip install gymnasium[box2d]\n","!pip install imageio\n","!pip install opencv-python\n","!pip install torch\n","!pip install matplotlib"]},{"cell_type":"markdown","metadata":{"id":"XnGglegdULOm"},"source":["### 2. Imports and Environment Wrapper\n","\n","Here, we import all the necessary libraries and define the `Env` class, which acts as a wrapper around the standard Gym environment. This class handles crucial preprocessing steps like converting images to grayscale, resizing, and stacking frames to give the agent a sense of motion."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"txDS5HB9ULOm"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torch.distributions import Beta, Categorical # Import Categorical for discrete actions\n","import torch.optim as optim\n","from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n","import gymnasium as gym\n","import numpy as np\n","import cv2\n","from collections import deque, namedtuple\n","import os\n","import time\n","import imageio\n","import matplotlib.pyplot as plt\n","\n","\n","class Env:\n","    \"\"\"\n","    Environment wrapper for CarRacing-v3.\n","    Includes preprocessing, frame stacking, and action space discretization.\n","    \"\"\"\n","    def __init__(self, frame_stack=4, action_repeat=4):\n","        self.env = gym.make('CarRacing-v3', continuous=True)\n","        self.action_repeat = action_repeat\n","        self.frame_stack = frame_stack\n","\n","        # --- NEW: Define the discrete action space as requested ---\n","        self.action_space = [\n","            [-1.0, 0.0, 0.0],   # 0: Turn Left\n","            [1.0, 0.0, 0.0],    # 1: Turn Right\n","            [0.0, 0.0, 0.8],    # 2: Brake\n","            [0.0, 1.0, 0.0],    # 3: Accelerate\n","            [0.0, 0.0, 0.0],    # 4: Do-Nothing\n","        ]\n","        self.num_actions = len(self.action_space)\n","\n","    def reset(self):\n","        \"\"\" Resets the environment and returns the initial stacked state. \"\"\"\n","        self.counter = 0\n","        self.die = False\n","        img_rgb, _ = self.env.reset()\n","        img_gray = self.rgb2gray(img_rgb)\n","        self.stack = deque([img_gray] * self.frame_stack, maxlen=self.frame_stack)\n","        return np.array(self.stack)\n","\n","    def step(self, action_index):\n","        \"\"\"\n","        Takes an action index, maps it to a continuous action, and steps the environment.\n","        \"\"\"\n","        # Get the continuous action from our discrete list and convert to numpy array\n","        continuous_action = np.array(self.action_space[action_index])\n","\n","        total_reward = 0\n","        for i in range(self.action_repeat):\n","            img_rgb, reward, terminated, truncated, info = self.env.step(continuous_action)\n","\n","            # Use the game's default reward (positive for visiting new tiles)\n","            total_reward += reward\n","\n","            # Penalty for being on the grass\n","            is_on_grass = np.mean(img_rgb[84:, 12:84, 1]) > 180\n","            if is_on_grass:\n","                total_reward -= 0.2\n","\n","            if terminated:\n","                self.die = True\n","                # Large penalty for crashing\n","                total_reward -= 100\n","                break\n","\n","        img_gray = self.rgb2gray(img_rgb)\n","        self.stack.append(img_gray)\n","\n","        done = self.die or self.counter > 1000 or truncated\n","        self.counter += 1\n","        return np.array(self.stack), total_reward, done, {}\n","\n","    def render(self, *args, **kwargs):\n","        \"\"\"Renders the environment.\"\"\"\n","        return self.env.render(*args, **kwargs)\n","\n","    @staticmethod\n","    def rgb2gray(rgb, norm=True):\n","        \"\"\" Converts an RGB image to grayscale (84x84) and normalizes it. \"\"\"\n","        gray = cv2.cvtColor(rgb, cv2.COLOR_RGB2GRAY)\n","        gray = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_AREA)\n","        if norm:\n","            gray = gray.astype(np.float32) / 255.\n","        return gray\n"]},{"cell_type":"markdown","metadata":{"id":"C8fS2grZULOn"},"source":["### 3. PPO Agent Implementation\n","\n","This section contains the core of the PPO algorithm. We define the `Actor` and `Critic` neural network architectures and the `PPOAgent` class that brings them together. The agent class handles action selection, storing experiences, and updating the networks based on the PPO loss function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jm0Xjj-nULOn"},"outputs":[],"source":["\n","\n","Transition = namedtuple('Transition', ['state', 'action', 'a_log_p', 'reward', 'next_state'])\n","\n","class Actor(nn.Module):\n","    \"\"\" Actor Network - Modified for Discrete Actions \"\"\"\n","    def __init__(self, num_actions):\n","        super(Actor, self).__init__()\n","        self.cnn_base = nn.Sequential(\n","            nn.Conv2d(4, 8, kernel_size=4, stride=2),\n","            nn.ReLU(),\n","            nn.Conv2d(8, 16, kernel_size=3, stride=2),\n","            nn.ReLU(),\n","            nn.Conv2d(16, 32, kernel_size=3, stride=2),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, kernel_size=3, stride=2),\n","            nn.ReLU(),\n","            nn.Flatten()\n","        )\n","        self.v = nn.Sequential(\n","            nn.Linear(1024, 100),\n","            nn.ReLU(),\n","            # --- NEW: Output layer for discrete action probabilities ---\n","            nn.Linear(100, num_actions)\n","        )\n","\n","    def forward(self, state):\n","        x = self.cnn_base(state)\n","        # --- NEW: Returns logits for the categorical distribution ---\n","        action_logits = self.v(x)\n","        return action_logits\n","\n","class Critic(nn.Module):\n","    \"\"\" Critic Network (Value Function) - Unchanged \"\"\"\n","    def __init__(self):\n","        super(Critic, self).__init__()\n","        self.cnn_base = nn.Sequential(\n","            nn.Conv2d(4, 8, kernel_size=4, stride=2),\n","            nn.ReLU(),\n","            nn.Conv2d(8, 16, kernel_size=3, stride=2),\n","            nn.ReLU(),\n","            nn.Conv2d(16, 32, kernel_size=3, stride=2),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, kernel_size=3, stride=2),\n","            nn.ReLU(),\n","            nn.Flatten()\n","        )\n","        self.v = nn.Sequential(nn.Linear(1024, 100), nn.ReLU(), nn.Linear(100, 1))\n","\n","    def forward(self, state):\n","        x = self.cnn_base(state)\n","        value = self.v(x)\n","        return value\n","\n","class PPOAgent:\n","    \"\"\" PPO Agent - Modified for Discrete Actions \"\"\"\n","    def __init__(self, device, num_actions):\n","        self.device = device\n","        self.actor_net = Actor(num_actions).float().to(self.device)\n","        self.critic_net = Critic().float().to(self.device)\n","        self.buffer = []\n","        self.batch_size = 64\n","        self.ppo_update_time = 10\n","        self.max_grad_norm = 0.5\n","        self.clip_param = 0.2\n","        self.gamma = 0.99\n","        self.lr = 0.0001\n","\n","        self.optimizer_actor = optim.Adam(self.actor_net.parameters(), lr=self.lr)\n","        self.optimizer_critic = optim.Adam(self.critic_net.parameters(), lr=self.lr)\n","\n","    def select_action(self, state):\n","        \"\"\" Selects a discrete action index using the actor network. \"\"\"\n","        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n","        with torch.no_grad():\n","            action_logits = self.actor_net(state)\n","\n","        # --- NEW: Use Categorical distribution for discrete actions ---\n","        dist = Categorical(logits=action_logits)\n","        action = dist.sample()\n","        action_log_prob = dist.log_prob(action)\n","\n","        return action.item(), action_log_prob.item()\n","\n","    def get_value(self, state):\n","        \"\"\" Gets the value of a state from the critic network. \"\"\"\n","        state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n","        with torch.no_grad():\n","            value = self.critic_net(state)\n","        return value.item()\n","\n","    def store_transition(self, transition):\n","        \"\"\" Stores a transition in the buffer. \"\"\"\n","        self.buffer.append(transition)\n","\n","    def update(self):\n","        \"\"\" Updates the actor and critic networks using the PPO algorithm. \"\"\"\n","        state = torch.tensor(np.array([t.state for t in self.buffer]), dtype=torch.float).to(self.device)\n","        action = torch.tensor([t.action for t in self.buffer], dtype=torch.int64).view(-1, 1).to(self.device)\n","        reward = [t.reward for t in self.buffer]\n","        old_action_log_prob = torch.tensor([t.a_log_p for t in self.buffer], dtype=torch.float).view(-1, 1).to(self.device)\n","\n","        R = 0\n","        Gt = []\n","        for r in reward[::-1]:\n","            R = r + self.gamma * R\n","            Gt.insert(0, R)\n","        Gt = torch.tensor(Gt, dtype=torch.float).to(self.device)\n","\n","        for i in range(self.ppo_update_time):\n","            for index in BatchSampler(SubsetRandomSampler(range(len(self.buffer))), self.batch_size, False):\n","                Gt_index = Gt[index].view(-1, 1)\n","                V = self.critic_net(state[index])\n","                delta = Gt_index - V\n","                advantage = delta.detach()\n","\n","                # --- NEW: Get log probabilities from the discrete distribution ---\n","                action_logits = self.actor_net(state[index])\n","                dist = Categorical(logits=action_logits)\n","                action_log_prob = dist.log_prob(action[index].squeeze()).view(-1, 1)\n","\n","                ratio = torch.exp(action_log_prob - old_action_log_prob[index])\n","\n","                surr1 = ratio * advantage\n","                surr2 = torch.clamp(ratio, 1 - self.clip_param, 1 + self.clip_param) * advantage\n","\n","                loss_actor = -torch.min(surr1, surr2).mean()\n","                loss_critic = nn.functional.mse_loss(Gt_index, V)\n","                entropy = dist.entropy().mean()\n","                loss_total = loss_actor + 0.5 * loss_critic - 0.05 * entropy\n","\n","                self.optimizer_actor.zero_grad()\n","                self.optimizer_critic.zero_grad()\n","                loss_total.backward()\n","                nn.utils.clip_grad_norm_(self.actor_net.parameters(), self.max_grad_norm)\n","                nn.utils.clip_grad_norm_(self.critic_net.parameters(), self.max_grad_norm)\n","                self.optimizer_actor.step()\n","                self.optimizer_critic.step()\n","\n","        del self.buffer[:]\n","\n","    def save_param(self, directory='./ppo_params'):\n","        \"\"\"Saves the model parameters.\"\"\"\n","        if not os.path.exists(directory):\n","            os.makedirs(directory)\n","        torch.save(self.actor_net.state_dict(), os.path.join(directory, 'actor.pth'))\n","        torch.save(self.critic_net.state_dict(), os.path.join(directory, 'critic.pth'))\n","\n","    def load_param(self, directory='./ppo_params'):\n","        \"\"\"Loads the model parameters.\"\"\"\n","        actor_path = os.path.join(directory, 'actor.pth')\n","        critic_path = os.path.join(directory, 'critic.pth')\n","        if os.path.exists(actor_path) and os.path.exists(critic_path):\n","            self.actor_net.load_state_dict(torch.load(actor_path, map_location=self.device))\n","            self.critic_net.load_state_dict(torch.load(critic_path, map_location=self.device))\n","            print(\"--- Parameters Loaded ---\")\n","        else:\n","            print(\"--- No Parameters Found, Starting from Scratch ---\")\n"]},{"cell_type":"markdown","metadata":{"id":"-UhBC0m3ULOo"},"source":["### 4. Training the Agent\n","\n","This is the main execution block. It initializes the environment and the agent, then enters the training loop. The agent collects experience, and once enough data is gathered in the buffer, it calls the `update` method to improve its policy. Progress is logged periodically."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uKSMfPxnULOo","outputId":"55ff9bfb-67b1-4367-f4f7-c206d1453173"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","--- Parameters Loaded ---\n","Episode 10\tAvg length: 124\tAvg reward: 67\n","--- Saving checkpoint --- \n","--- Parameters Saved ---\n","Episode 20\tAvg length: 124\tAvg reward: 68\n","--- Saving checkpoint --- \n","--- Parameters Saved ---\n","Episode 30\tAvg length: 124\tAvg reward: 77\n","--- Saving checkpoint --- \n","--- Parameters Saved ---\n","Episode 40\tAvg length: 124\tAvg reward: 71\n","--- Saving checkpoint --- \n","--- Parameters Saved ---\n","Episode 50\tAvg length: 124\tAvg reward: 89\n","--- Saving checkpoint --- \n","--- Parameters Saved ---\n","Episode 60\tAvg length: 124\tAvg reward: 75\n","--- Saving checkpoint --- \n","--- Parameters Saved ---\n","Episode 70\tAvg length: 124\tAvg reward: 68\n","--- Saving checkpoint --- \n","--- Parameters Saved ---\n","Episode 80\tAvg length: 124\tAvg reward: 76\n","--- Saving checkpoint --- \n","--- Parameters Saved ---\n","Episode 90\tAvg length: 124\tAvg reward: 73\n","--- Saving checkpoint --- \n","--- Parameters Saved ---\n"]}],"source":["# --- Main Training Loop ---\n","\n","# Global list to store rewards for plotting\n","reward_history = []\n","\n","def main():\n","    # Configuration\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print(f\"Using device: {device}\")\n","\n","    env = Env()\n","    agent = PPOAgent(device, num_actions=env.num_actions)\n","\n","    # Load pre-trained models if they exist\n","    agent.load_param()\n","\n","    # Training parameters\n","    max_episodes = 2000\n","    update_timestep = 2048 # Update policy every N timesteps\n","    log_interval = 10 # Print avg reward in the interval\n","\n","    running_reward = 0\n","    avg_length = 0\n","    timestep = 0\n","\n","    # Training loop\n","    for i_episode in range(1, max_episodes + 1):\n","        state = env.reset()\n","        ep_reward = 0\n","\n","        for t in range(1001): # Max timesteps per episode\n","            timestep += 1\n","\n","            # Select action and step\n","            action_index, action_log_prob = agent.select_action(state)\n","            next_state, reward, done, _ = env.step(action_index)\n","\n","            # Store transition\n","            trans = Transition(state, action_index, action_log_prob, reward, next_state)\n","            agent.store_transition(trans)\n","\n","            state = next_state\n","            ep_reward += reward\n","\n","            # Update if buffer is full\n","            if len(agent.buffer) >= update_timestep:\n","                agent.update()\n","\n","            if done:\n","                break\n","\n","        running_reward += ep_reward\n","        avg_length += t\n","\n","        # Logging\n","        if i_episode % log_interval == 0:\n","            avg_length = int(avg_length / log_interval)\n","            running_reward = int((running_reward / log_interval))\n","\n","            # Store reward for plotting\n","            reward_history.append(running_reward)\n","\n","            print(f'Episode {i_episode}\\tAvg length: {avg_length}\\tAvg reward: {running_reward}')\n","\n","            # Save a checkpoint of the model\n","            print(\"--- Saving checkpoint --- \")\n","            agent.save_param()\n","\n","            running_reward = 0\n","            avg_length = 0\n","\n","# Start training\n","main()"]},{"cell_type":"markdown","metadata":{"id":"d68ZgdnjULOp"},"source":["### 5. Evaluate and Record a Demo\n","\n","After training, this final block loads the saved model parameters and runs the agent for 10 episodes to evaluate its performance on unseen tracks. It also records a video of the first evaluation episode, which can be used as the demo deliverable."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ioBfdJlyULOp"},"outputs":[],"source":["# --- Plotting, Evaluation and Recording ---\n","\n","def plot_rewards():\n","    \"\"\"Plots the training reward history.\"\"\"\n","    plt.figure(figsize=(10, 5))\n","    plt.plot(np.arange(len(reward_history)) * 10, reward_history)\n","    plt.xlabel('Episode')\n","    plt.ylabel('Average Reward')\n","    plt.title('Training Reward Curve')\n","    plt.grid(True)\n","    plt.show()\n","\n","def evaluate_and_record():\n","    \"\"\"\n","    Evaluates the trained agent and records a video of its performance.\n","    \"\"\"\n","    # To render in Colab, we need a virtual display\n","    !pip install pyvirtualdisplay > /dev/null 2>&1\n","    !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n","    from pyvirtualdisplay import Display\n","    display = Display(visible=0, size=(1400, 900))\n","    display.start()\n","\n","    device = torch.device(\"cpu\") # Use CPU for evaluation\n","    env = Env()\n","    # IMPORTANT: Make sure the environment for evaluation has rendering enabled\n","    env.env = gym.make('CarRacing-v3', continuous=True, render_mode='rgb_array')\n","    agent = PPOAgent(device, num_actions=env.num_actions)\n","    agent.load_param()\n","\n","    video_filename = \"ppo_carracing_evaluation.mp4\"\n","    frames = []\n","\n","    print(\"--- Plotting Reward History ---\")\n","    plot_rewards()\n","\n","    print(\"\\n--- Starting Evaluation ---\")\n","    total_reward = 0\n","    for i in range(10): # Evaluate over 10 episodes\n","        state = env.reset()\n","        episode_reward = 0\n","        for t in range(1001):\n","            action_index, _ = agent.select_action(state)\n","            state, reward, done, _ = env.step(action_index)\n","\n","            # For recording, render the environment\n","            if i == 0: # Record the first episode\n","                try:\n","                    frame = env.render()\n","                    frames.append(frame)\n","                except Exception as e:\n","                    print(f\"Rendering failed: {e}. Skipping frame.\")\n","\n","            episode_reward += reward\n","            if done:\n","                break\n","        print(f\"Episode {i+1}: Reward = {episode_reward:.2f}\")\n","        total_reward += episode_reward\n","\n","    avg_reward = total_reward / 10\n","    print(f\"\\n--- Average Reward over 10 episodes: {avg_reward:.2f} ---\")\n","\n","    # Save the video\n","    if frames:\n","        print(f\"--- Saving video to {video_filename} ---\")\n","        imageio.mimsave(video_filename, frames, fps=30)\n","\n","# Run evaluation after training\n","# evaluate_and_record()\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}