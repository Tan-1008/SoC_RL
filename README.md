Week 3 : This week was all about starting the basics of RL. I followed the David Silver lectures on youtube to understand about RL. I learnt about what a typical RL system looks like, involving an evnironment and agent. I learnt about basic RL terms like reward, state, return and action. I also understood the math behind Markov Decision Processes, and various methods about how they can be solved, like dynamic programming, Monte Carlo methods and temporal difference learning. Also looked at some classic examples of implementation of RL in beating some Atari games.

Week 4 : I used this week to learn about the concepts of Q-learning, a fundamental reinforcement learning algorithm. Q-learning is a model-free method where an agent learns to make optimal decisions by interacting with its environment and updating a table of Q-valuesâ€”each representing the expected future reward of taking a certain action in a given state. A key part of this process is the Bellman equation, which helps the agent refine its predictions over time based on new experiences. I also learned about the importance of balancing exploration (trying new actions) and exploitation (choosing the best-known action), typically handled using an epsilon-greedy policy. Other important concepts include the learning rate (which controls how much the Q-values are updated) and the discount factor (which determines how much future rewards are valued). 
